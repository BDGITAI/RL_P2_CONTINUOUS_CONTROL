{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control with 20 agents\n",
    "\n",
    "---\n",
    "\n",
    "## Preamble\n",
    "\n",
    "### The project\n",
    "\n",
    "In this notebook, we explore the use of parallel training of agents performing the same tasks. We collect the learning from all the agent and update an Actor-Critic network that represents the knowledge gathered by the different agents. A ll agents DDPG algorithm is used to solve a continuous control environment for the  [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).\n",
    "\n",
    "Before using this notebook check that you have followed the .Readme file available in [GitHub Project repository](https://github.com/BDGITAI/RL_P2_CONTINUOUS_CONTROL)\n",
    "\n",
    "For the Notebook to work you will need the Reacher environment executable (for 20 agents) which was placed in [GitHub Project repository](https://github.com/BDGITAI/RL_P2_CONTINUOUS_CONTROL/Reacher_Windows_x86_64/). The environment needs to uncompressed as  `\"./Reacher_windows_x86_64_20/Reacher.exe\"`\n",
    "\n",
    "This implementation uses the Pytorch library and was tested in a **Windows 64 bits** platform using **CPU**  computation. \n",
    "\n",
    "\n",
    "This notebook is divided in two parts\n",
    "* **Part 1** : Training. We will train an Agent and see how the learning can be applied to execute a task\n",
    "* **Part 2** : To see an already trained agent. You ll need to run the section 1.1 to get the import done but can skip the training section\n",
    "* **Part 3** : To evaluate a trained agent over 100 episodes. You ll need to run the section 1.1 to get the import done but can skip the training section\n",
    "\n",
    "\n",
    "### Base used for the project\n",
    "\n",
    "[Shangtong Zhang](https://github.com/ShangtongZhang) implemented several agents and made the implementations in its GITHUB repository. This project is based on its work that was introduced in the Udacity's lectures.  \n",
    "The base was created to interact with openai and atari environment.\n",
    "This notebook highlights the modifications allowing the interactions with the Unity environment through the python API.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Part 1 : Training 20 agents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "zip_ref = zipfile.ZipFile('./data/Reacher_Linux_NoVis.zip', 'r')\n",
    "zip_ref.extractall('./data/Reacher_Linux_NoVis/')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import stat\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk('.'):\n",
    "    for filename in filenames:\n",
    "        path = os.path.join(dirpath, filename)\n",
    "        os.chmod(path, 0o777) # for example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Imports and initialisation function\n",
    "Import the deep_rl libraries from Shangtong and create a function creating the DDPG agent interacting with the Unity environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial approach would be to create 20 DDPG agents each one having their memory and actor critic network. Each agent could be trained in parallel for a given amount of timesteps. Then contents of all memories could be merged , one central network would be updated with the learning (forward and barkward pass). The resulting weights would then be copied into the 20 agents.\n",
    "    1. Create 20 agents (threads)\n",
    "    2. Each agents interact with environmnet for T_TIMESTEPS\n",
    "    3. Once all thread are waiting, merge all memories into a central memory\n",
    "    4. Perform Actor Critic Network update\n",
    "    5. Update all agents with new weights\n",
    "    6. Go to step 2 and repeat until end of episode\n",
    "\n",
    "\n",
    "However, the unity environmnent contains the environment for all 20 agents. It implies that all 20 actions need to be given at the same time to update unity and get new states. This limits the use of thread to create parallel tasks.\n",
    "Instead, the following cells create 20 agents that are called consecutively from the training loop. All share the same replay memory and network. The training first determine what actions are taken and uses the new states to update all agents.\n",
    "\n",
    "To enable the training a new DDPG20 class is created to slit the step function in two parts : evaluation action , step the environmnent (see [DDPG.py](https://github.com/BDGITAI/RL_P2_CONTINUOUS_CONTROL/blob/master/deep_rl/agent/DDPG_agent.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from deep_rl import *\n",
    "\n",
    "#\n",
    "# Create a the Actor Critic Network that will be shared\n",
    "# by all agents for evaluating actions. The unity environment requires all actions\n",
    "# to be passed at once. Hence we can evaluate actions for the 20 agents successively\n",
    "# and then step the environment. The unity env creates a bottleneck that limits the use\n",
    "# of parallelism. With successive call there will be no competitive access to the network\n",
    "# while training\n",
    "# \n",
    "\n",
    "def shared_net(state_dim,action_dim):\n",
    "     return (DeterministicActorCriticNet(\n",
    "        state_dim, action_dim,\n",
    "        actor_body=FCBody(state_dim, (400, 300), gate=F.tanh),\n",
    "        critic_body=TwoLayerFCBodyWithAction(\n",
    "            state_dim, action_dim, (400, 300), gate=F.tanh),\n",
    "        actor_opt_fn=lambda params: torch.optim.Adam(params, lr=1e-4),\n",
    "        critic_opt_fn=lambda params: torch.optim.Adam(params, lr=1e-3)))\n",
    "\n",
    "#\n",
    "# Create a shared memory between 20 agents. \n",
    "# The stepping function of all agents will be called in sequence avoiding competing\n",
    "# access when inserting or removing data from the memory\n",
    "#   \n",
    "def shared_mem():\n",
    "    return Replay(memory_size=int(1e7), batch_size=64)  \n",
    "\n",
    "\n",
    "#\n",
    "# Create multiple agents\n",
    "#\n",
    "def create_agents(num_agent,net,target_net,mem):\n",
    "    config = Config()\n",
    "    log_dir = get_default_log_dir(create_agents.__name__)\n",
    "    config.task_fn = lambda **kwargs: ContinuousControl(config.eval_env,log_dir=log_dir)\n",
    "    config.eval_env = Unity20()\n",
    "    config.max_steps = int(1e6)\n",
    "    config.eval_interval = int(1e4)\n",
    "    config.eval_episodes = 20\n",
    "    config.discount = 0.99\n",
    "    config.random_process_fn = lambda: OrnsteinUhlenbeckProcess(\n",
    "        size=(config.action_dim, ), std=LinearSchedule(0.2))\n",
    "    config.min_memory_size = 64\n",
    "    config.target_network_mix = 1e-3\n",
    "    #config.logger = get_logger()\n",
    "    agents = []\n",
    "    # create 20 agents \n",
    "    # the DDPG class is modified in a DDPG20 class to accomodate the training\n",
    "    for i in range(num_agent):\n",
    "        agents.append (DDPGAgent20(config,net,target_net,mem))\n",
    "    return agents         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(agents,n_episodes=2000, max_t=1000,score_target=30):\n",
    "    scores = []     # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        # reset environment at the start of each episode\n",
    "        states = agents[0].config.eval_env.reset()\n",
    "        for agent,state in zip(agents,states):\n",
    "            agent.state = state\n",
    "        # no reward at start\n",
    "        episode_rewards = np.zeros(len(agents))\n",
    "        ep_scores = np.zeros(len(agents))\n",
    "        for t in range(max_t):\n",
    "            # decide wich action to take according to eps greedy policy\n",
    "            actions = []\n",
    "            for agent in agents:\n",
    "                action = agent.half_step1()\n",
    "                actions.append(action)\n",
    "            # execute the action in the environment (interface with unity)\n",
    "            next_states, rewards, dones, infos = agent.config.eval_env.step(actions)\n",
    "            # store this experience in the memory and learn if enough experiences have been gathered\n",
    "            for agent,action,next_state,reward,done,info in zip(agents,actions,next_states, rewards, dones, infos):\n",
    "                agent.half_step2(action,next_state,reward,done,info)\n",
    "            # update current state as next state\n",
    "            for agent,state in zip(agents,next_states):\n",
    "                agent.state = state\n",
    "            # cumul reward\n",
    "            ep_scores += rewards\n",
    "            if np.any(dones):\n",
    "                break \n",
    "        score = np.mean(ep_scores)\n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tMax Score: {:.2f}'.format(i_episode, np.mean(scores_window),np.max(scores_window)), end=\"\")\n",
    "        # save network weigth every 100 episodes\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            path = 'checkpoint20_'+str(i_episode)+'.pth'\n",
    "            agent.save(path)\n",
    "            # save score\n",
    "            f = open('output20.txt', 'w')\n",
    "            f.write('score: '+ str(scores)+'\\n')\n",
    "            f.close()\n",
    "        # target is 30 to win. \n",
    "        if np.mean(scores_window)>=score_target:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            path = 'checkpoint20_'+str(i_episode)+'.pth'\n",
    "            agent.save(path)\n",
    "            # save score\n",
    "            f = open('output20.txt', 'w')\n",
    "            f.write('score: '+ str(scores)+'\\n')\n",
    "            f.close()\n",
    "            break\n",
    "    return scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Execute training loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Episode 100\tAverage Score: 11.97\tMax Score: 17.96\n",
      "Episode 131\tAverage Score: 16.07\tMax Score: 17.97\n",
      "Environment solved in 31 episodes!\tAverage Score: 16.07\n"
     ]
    }
   ],
   "source": [
    "# create default directories to maintain compatibility with the base\n",
    "mkdir('data/video')\n",
    "mkdir('dataset')\n",
    "mkdir('log')\n",
    "\n",
    "# train on CPU and not multi agent training\n",
    "set_one_thread()\n",
    "# train on CPU : -1 , GPU = 0\n",
    "select_device(-1)\n",
    "# create the agent\n",
    "                   \n",
    "# create a shared Network with observation dimensions = 33 and actions dimensions =4 (see .Readme)\n",
    "network = shared_net(33,4)\n",
    "# create a shared target Network with observation dimensions = 33 and actions dimensions =4 (see .Readme)\n",
    "target_network = shared_net(33,4)\n",
    "# create a shared replay memory to collect all data from all agents\n",
    "replay = shared_mem()\n",
    "# create agents\n",
    "agents = create_agents(20,network,target_network,replay)\n",
    "# start training , train on only half the required time steps to get different experiences quicker\n",
    "scores = train(agents, n_episodes=1000, max_t=500, score_target =16.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Plot the training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f82a9805dd8>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Part 2 : Watch a trained agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the part 1.1 of this notebook to import the libraries and function definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a shared Network with observation dimensions = 33 and actions dimensions =4 (see .Readme)\n",
    "network = shared_net(33,4)\n",
    "# create a shared target Network with observation dimensions = 33 and actions dimensions =4 (see .Readme)\n",
    "target_network = shared_net(33,4)\n",
    "# create a shared replay memory to collect all data from all agents\n",
    "replay = shared_mem()\n",
    "# create agents\n",
    "agents = create_agents(20,network,target_network,replay)\n",
    "# initialise weights with learned ones\n",
    "for agent in agents:\n",
    "    agent.load('./successful_weigths/reacher_20.pth')\n",
    "# Disable the training mode of the environmnet\n",
    "brain_name = agents[0].config.eval_env.env.brain_names[0]\n",
    "env_info = agents[0].config.eval_env.env.reset(train_mode=False)[brain_name] \n",
    "# reset states\n",
    "states = env_info.vector_observations\n",
    "scores = np.zeros(20)                        # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = []\n",
    "    for agent,state in zip(agents,states):\n",
    "        actions.append(agent.eval_step(state))\n",
    "    # execute the action in the environment (interface with unity)\n",
    "    env_info = agent.config.eval_env.env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += rewards                                  # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score this episode: {}'.format(np.mean(scores)))\n",
    "agent.config.eval_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Part 3 : Evaluate a trained agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the part 1.1 of this notebook to import the libraries and function definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def eval(agents,n_episodes=100):\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = agents[0].config.eval_env.env.reset(train_mode=False)[brain_name] \n",
    "        # reset states\n",
    "        states = env_info.vector_observations\n",
    "        scores = np.zeros(20)                        # initialize the score (for each agent)   \n",
    "        actions = []\n",
    "        for agent,state in zip(agents,states):\n",
    "            actions.append(agent.eval_step(state))\n",
    "        # execute the action in the environment (interface with unity)\n",
    "        env_info = agent.config.eval_env.env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += rewards                                  # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "        score = np.mean(ep_scores)\n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tMax Score: {:.2f}}'.format(i_episode, np.mean(scores_window),np.max(scores_window)), end=\"\")\n",
    "    return scores\n",
    "\n",
    "# create a shared Network with observation dimensions = 33 and actions dimensions =4 (see .Readme)\n",
    "network = shared_net(33,4)\n",
    "# create a shared target Network with observation dimensions = 33 and actions dimensions =4 (see .Readme)\n",
    "target_network = shared_net(33,4)\n",
    "# create a shared replay memory to collect all data from all agents\n",
    "replay = shared_mem()\n",
    "# create agents\n",
    "agents = create_agents(20,network,target_network,replay)\n",
    "# initialise weights with learned ones\n",
    "for agent in agents:\n",
    "    agent.load('./successful_weigths/reacher_20.pth')\n",
    "# Disable the training mode of the environmnet\n",
    "brain_name = agents[0].config.eval_env.env.brain_names[0]\n",
    "env_info = agents[0].config.eval_env.env.reset(train_mode=False)[brain_name]\n",
    "\n",
    "# evaluation\n",
    "scores = eval(agents)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()\n",
    "\n",
    "agents[0].config.eval_env.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
